# E2V Framework Implementation Guide

## 1. Executive Summary

This guide outlines the implementation approach for Elements-to-Video (E2V) training within the finetrainers framework. The E2V functionality enables training Wan models to generate videos from multiple reference images (elements) like characters, objects, and backgrounds.

Key architectural decisions:
- Implement E2V as a new training TYPE within the framework, not a new model
- Leverage existing WanControlModelSpecification without modification
- Process reference images through dual pathways: CLIP (semantic) and VAE (spatial)
- Follow the same wrapping patterns used by control training

The implementation introduces no new model parameters but adds specialized data processing for handling multiple reference images and combining them effectively for training.

## 2. Configuration Format

### 2.1 JSON Structure

```json
{
  "datasets": [
    {
      "data_root": "path/to/e2v/dataset", 
      "dataset_type": "e2v",
      "target_resolution": [480, 854],
      "auto_frame_buckets": true,
      "reshape_mode": "bicubic",
      "remove_common_llm_caption_prefixes": true,
      
      "elements": [
        {
          "name": "main_subject", 
          "suffixes": ["_dog.png", "_person.png", "_mask.png"],
          "required": true,
          "vae": {
            "repeat": 4, 
            "position": 0
          },
          "clip": {
            "preprocess": "center_crop"
          }
        },
        {
          "name": "secondary",
          "suffixes": ["_object.png", "_toy.png"],
          "required": false,
          "vae": {
            "repeat": 4, 
            "position": 1
          },
          "clip": {
            "preprocess": "pad_white"
          }
        }
      ],
      
      "processors": {
        "vae": {
          "resolution": [480, 854],
          "combine": "before",
          "default_preprocess": "resize",
          "frame_conditioning": "full",
          "frame_index": 0,
          "concatenate_mask": true
        },
        "clip": {
          "resolution": [512, 512],
          "default_preprocess": "center_crop"
        }
      },
      
      "visualization": {
        "enabled": true,
        "output_dir": "visualizations/{run_id}",
        "frequency": 100,
        "processors": [
          {
            "type": "latent_save",
            "data": ["input_latents", "predicted_latents"]
          },
          {
            "type": "vae_decode",
            "data": ["input_latents", "predicted_latents"],
            "frames": [0, -1]
          }
        ]
      }
    }
  ]
}
```

### 2.2 Benefits of JSON Format

1. **Clean Separation of Concerns**
   - Elements define what inputs to use and their basic properties
   - Processors define how to handle different pathways globally
   - Each element can override processor settings as needed

2. **Framework-Friendly Structure**
   - Direct mapping to processor names via the "processors" object
   - Simple element-processor parameter override pattern
   - Clear pathway for extending with new processor types

3. **Extensibility**
   - New elements can be added easily
   - New processor types can be added without changing the structure
   - Additional parameters can be added to elements or processors

### 2.3 Implementation Strategy

1. **Keep the Concise JSON Format**
   - Maintain the current structure which is user-friendly
   - Focus on human-readability in configuration files

2. **Add Conversion Logic**
   - Create mapping functions in the configuration classes
   - Use `map_from_json` methods to convert JSON to framework objects
   - Keep complexity in code rather than in configuration files

3. **Document Expected Parameters**
   - Clearly document required vs. optional parameters
   - Document default values for each parameter
   - Provide examples of common configurations

## 3. Framework Components

### 3.1 Configuration Classes

```python
class E2VType(str, Enum):
    """Enum for E2V processing types."""
    VAE = "vae"
    CLIP = "clip"
    DUAL = "dual"  # Both pathways enabled

class FrameConditioningType(str, Enum):
    """Reused from control training."""
    INDEX = "index"
    PREFIX = "prefix"
    RANDOM = "random"
    FIRST_AND_LAST = "first_and_last"
    FULL = "full"

class ElementConfig(ConfigMixin):
    """Configuration for a single element."""
    name: str
    suffixes: List[str]
    required: bool = False
    vae: Dict[str, Any] = {"repeat": 1, "position": 0}
    clip: Union[Dict[str, Any], bool] = {"preprocess": "center_crop"}
    
    def validate_args(self, args):
        assert isinstance(self.name, str), "Element name must be a string"
        assert isinstance(self.suffixes, list), "Suffixes must be a list"
        assert all(isinstance(s, str) for s in self.suffixes), "All suffixes must be strings"

class ProcessorConfig(ConfigMixin):
    """Base configuration for processors."""
    resolution: List[int]
    default_preprocess: str = "resize"
    
    def validate_args(self, args):
        assert len(self.resolution) == 2, "Resolution must be [height, width]"

class VaeProcessorConfig(ProcessorConfig):
    """Configuration for VAE pathway."""
    combine: str = "before"
    frame_conditioning: str = FrameConditioningType.FULL
    frame_index: int = 0
    concatenate_mask: bool = True

class ClipProcessorConfig(ProcessorConfig):
    """Configuration for CLIP pathway."""
    default_preprocess: str = "center_crop"

class E2VLowRankConfig(ConfigMixin):
    """Configuration for E2V training with LoRA."""
    elements: List[ElementConfig]
    processors: Dict[str, Union[VaeProcessorConfig, ClipProcessorConfig]]
    rank: int = 64
    lora_alpha: int = 64
    target_modules: Union[str, List[str]] = "(transformer_blocks|single_transformer_blocks).*(to_q|to_k|to_v|to_out.0|ff.net.0.proj|ff.net.2)"
    train_qk_norm: bool = False
    
    def validate_args(self, args):
        assert len(self.elements) > 0, "At least one element must be specified"
        assert "vae" in self.processors, "VAE processor configuration is required"
        assert "clip" in self.processors, "CLIP processor configuration is required"
    
    def map_from_json(self, json_config):
        """Map from JSON config to this class."""
        # Implementation details
```

### 3.2 Processor Classes

```python
class VAEPathwayProcessor(ProcessorMixin):
    """Processor for the VAE spatial pathway."""
    
    def __init__(self, output_names=None, input_names=None, config=None, device=None):
        super().__init__()
        self.output_names = output_names or ["vae_output"]
        self.input_names = input_names or {}
        self.config = config
        self.device = device
        assert len(self.output_names) == 1
    
    def forward(self, image=None, video=None, element_config=None, **kwargs):
        """Process image/video through VAE pathway.
        
        Args:
            image: Optional image tensor (B, C, H, W)
            video: Optional video tensor (B, F, C, H, W)
            element_config: Configuration for this element
            
        Returns:
            Dictionary with processed VAE output
        """
        # 1. Get configuration with element-specific overrides
        config = {**self.config}
        if element_config and "vae" in element_config:
            config.update(element_config["vae"])
        
        # 2. Preprocess image/video
        processed = self._preprocess_input(image, video, config)
        
        # 3. Apply repetition based on config
        repeated = self._apply_repetition(processed, config.get("repeat", 1))
        
        # 4. Run VAE encoder
        encoded = self._encode_vae(repeated)
        
        # Result includes position for later concatenation
        result = {
            "latents": encoded,
            "position": config.get("position", 0),
            "frames": repeated.shape[1] if hasattr(repeated, "shape") else 1
        }
        
        return {self.output_names[0]: result}
    
    def _preprocess_input(self, image, video, config):
        # Implementation details
        pass
    
    def _apply_repetition(self, image, repeat):
        # Implementation details
        pass
    
    def _encode_vae(self, video):
        # Implementation details
        pass

class CLIPPathwayProcessor(ProcessorMixin):
    """Processor for the CLIP semantic pathway."""
    
    def __init__(self, output_names=None, input_names=None, config=None, device=None):
        super().__init__()
        self.output_names = output_names or ["clip_output"]
        self.input_names = input_names or {}
        self.config = config
        self.device = device
        assert len(self.output_names) == 1
    
    def forward(self, image=None, element_config=None, **kwargs):
        """Process image through CLIP pathway.
        
        Args:
            image: Image tensor (B, C, H, W)
            element_config: Configuration for this element
            
        Returns:
            Dictionary with processed CLIP features
        """
        # 1. Get configuration with element-specific overrides
        config = {**self.config}
        if element_config and "clip" in element_config:
            if isinstance(element_config["clip"], dict):
                config.update(element_config["clip"])
            elif not element_config["clip"]:
                # CLIP pathway disabled for this element
                return {self.output_names[0]: None}
        
        # 2. Preprocess image
        processed = self._preprocess_input(image, config)
        
        # 3. Run CLIP encoder
        features = self._run_clip_encoder(processed)
        
        return {self.output_names[0]: features}
    
    def _preprocess_input(self, image, config):
        # Implementation details
        pass
    
    def _run_clip_encoder(self, image):
        # Implementation details
        pass
```

### 3.3 Dataset Implementation

```python
class IterableE2VDataset(torch.utils.data.IterableDataset, torch.distributed.checkpoint.stateful.Stateful):
    """Dataset wrapper for E2V training."""
    
    def __init__(self, dataset, config, device=None):
        super().__init__()
        self.dataset = dataset
        self.config = config
        self.device = device
        
        # Initialize processors
        self.processors = {
            "vae": VAEPathwayProcessor(
                output_names=["vae_output"],
                config=config["processors"]["vae"],
                device=device
            ),
            "clip": CLIPPathwayProcessor(
                output_names=["clip_output"],
                config=config["processors"]["clip"],
                device=device
            )
        }
        
        # Create element lookup
        self.elements = {elem["name"]: elem for elem in config["elements"]}
        
        logger.info("Initialized IterableE2VDataset")
    
    def __iter__(self):
        logger.info("Starting IterableE2VDataset")
        for data in iter(self.dataset):
            # Find element files
            element_files = self._find_element_files(data)
            
            # Process elements through pathways
            processed_data = self._process_elements(data, element_files)
            
            # Combine pathways
            combined_data = self._combine_pathways(processed_data)
            
            yield combined_data
    
    def load_state_dict(self, state_dict):
        self.dataset.load_state_dict(state_dict)
    
    def state_dict(self):
        return self.dataset.state_dict()
    
    def _find_element_files(self, data):
        """Find files for each element based on suffixes."""
        # Implementation details
        pass
    
    def _process_elements(self, data, element_files):
        """Process each element through each pathway."""
        results = {"vae": {}, "clip": {}}
        
        for element_name, files in element_files.items():
            element_config = self.elements[element_name]
            
            # Load element image
            element_image = self._load_image(files["path"])
            
            # Process through each pathway
            for processor_name, processor in self.processors.items():
                if processor_name in element_config and element_config[processor_name]:
                    result = processor(
                        image=element_image,
                        element_config=element_config
                    )
                    
                    # Store result if pathway is enabled
                    if result[processor.output_names[0]] is not None:
                        results[processor_name][element_name] = result[processor.output_names[0]]
        
        return results
    
    def _combine_pathways(self, processed_data):
        """Combine results from all pathways."""
        # Implementation based on apply_frame_conditioning_on_latents
        # Details similar to A2 pipeline
        pass
```

### 3.4 Trainer Implementation

```python
class E2VTrainer:
    """Trainer for E2V fine-tuning."""
    
    def __init__(self, config, model_spec=None):
        self.config = config
        self.model_spec = model_spec or WanControlModelSpecification(...)
        
        # Setup dataset
        self.train_dataset = IterableE2VDataset(
            dataset=self._create_base_dataset(),
            config=config["datasets"][0],
            device=self.device
        )
        
        # Rest of initialization
    
    def _train_step(self, batch):
        """Execute a single training step."""
        # 1. Extract pathways
        vae_latents = batch["vae_combined"]
        clip_features = batch["clip_combined"]
        video_latents = batch["video_latents"]
        text_embeddings = batch["text_embeddings"]
        
        # 2. Forward pass using model specification
        loss = self.model_spec.forward(
            transformer=self.transformer,
            condition_model_conditions={
                "encoder_hidden_states": text_embeddings,
                "encoder_hidden_states_image": clip_features
            },
            latent_model_conditions={
                "latents": video_latents,
                "control_latents": vae_latents
            },
            sigmas=self.sigmas
        )
        
        # 3. Backprop and optimize
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return loss.item()
    
    # Additional methods following framework patterns
```

## 4. Data Flow Diagram

### 4.1 End-to-End Pipeline

```
Load Data
└─> Base Dataset
    └─> IterableE2VDataset (wrapper)
        ├─> Find Element Files
        │   ├─> main_subject: 001_person.png
        │   ├─> secondary: 001_object.png
        │   └─> environment: 001_background.png
        │
        ├─> Process Elements
        │   ├─> VAEPathwayProcessor (SPATIAL PATH)
        │   │   ├─> Preprocess each element
        │   │   ├─> Apply repetition (mini-video creation)
        │   │   ├─> Encode through VAE
        │   │   └─> Store with position metadata
        │   │
        │   └─> CLIPPathwayProcessor (SEMANTIC PATH)
        │       ├─> Preprocess each element
        │       ├─> Run CLIP encoder
        │       └─> Store feature embeddings
        │
        └─> Combine Pathways
            ├─> VAE Pathway
            │   ├─> Sort elements by position
            │   ├─> Concatenate temporal dimension
            │   ├─> Create frame mask
            │   └─> Concatenate mask with latents
            │
            └─> CLIP Pathway
                └─> Concatenate all embeddings

Forward Pass (inside E2VTrainer._train_step)
├─> condition_model_conditions
│   ├─> encoder_hidden_states: Text embeddings
│   └─> encoder_hidden_states_image: CLIP features
│
└─> latent_model_conditions
    ├─> latents: Target video latents
    └─> control_latents: Combined VAE pathway
```

### 4.2 Key Data Transformations

1. **Elements → VAE Pathway**
   - Each image is preprocessed to target resolution
   - Images are arranged in sequence according to position parameters
   - Each image is repeated N times (according to vae_repeat)
   - The sequence is encoded through VAE as a mini-video
   - A mask tensor identifies which frames contain reference images
   - Mask and encoded sequence are concatenated along channel dimension

2. **Elements → CLIP Pathway**
   - Each image is preprocessed to clip_resolution
   - Images are passed through CLIP vision encoder
   - Resulting features are concatenated
   - Features are passed via encoder_hidden_states_image parameter

3. **Target Video Processing**
   - Video is encoded through VAE
   - Video latents have noise applied (Flow Matching)
   - VAE reference conditioning is combined with video latents
   - Combined tensor is passed to transformer

## 5. Implementation Plan

### 5.1 Step-by-Step Implementation

1. **Create Configuration Classes** (Day 1)
   - Define E2VType, ElementConfig, ProcessorConfig classes
   - Create mapping from JSON config to framework classes
   - Add E2V training types to TrainingType enum

2. **Implement Processors** (Days 2-3)
   - Create VAEPathwayProcessor extending ProcessorMixin
   - Create CLIPPathwayProcessor extending ProcessorMixin
   - Ensure both follow the framework's processor patterns
   - Register processors in the framework

3. **Create Dataset Wrapper** (Days 4-5)
   - Implement IterableE2VDataset to wrap base datasets
   - Add support for element file finding and loading
   - Implement pathway processing and combination
   - Create ValidationE2VDataset with similar structure

4. **Build Trainer** (Days 6-7)
   - Extend existing trainer classes with E2V-specific logic
   - Integrate with model specification
   - Implement visualization handling
   - Add configuration handling

5. **Testing** (Days 8-10)
   - Create unit tests for processors
   - Create integration tests for dataset + processors
   - Test with sample data
   - Validate against A2 inference outputs

### 5.2 Dependencies and Critical Path

Critical dependencies:
1. Configuration classes must be implemented first
2. Processors depend on configuration classes
3. Dataset implementation depends on processors
4. Trainer implementation depends on dataset
5. Testing depends on all components

## 6. Framework Compatibility

### 6.1 Alignment with Framework Patterns

1. **Configuration Classes**
   - Our E2VType enum follows the same pattern as ControlType in the existing code
   - Our config classes extend ConfigMixin just like ControlLowRankConfig and ControlFullRankConfig
   - We include the same validation and mapping methods seen in other configs

2. **Processor Implementation**
   - Our processors extend ProcessorMixin just like CannyProcessor and others
   - We follow the same pattern for input/output names handling
   - The forward method signature matches the framework's processor pattern

3. **Dataset Wrappers**
   - Our IterableE2VDataset follows the same structure as IterableControlDataset
   - We implement the same stateful interface for checkpoint compatibility
   - The data processing flow matches the control training pattern

4. **Trainer Implementation**
   - Our trainer follows the same structure as the existing trainers
   - We leverage the same model specification approach
   - The train step implementation follows the same pattern

### 6.2 Comparison with Control Training

Our approach is directly parallel to control training:

- Control training wraps datasets to add control signal processing
- E2V training wraps datasets to add element processing
- Both use specialized processors for different types of conditioning
- Both leverage the same underlying model specification

The only significant difference is in how we handle multiple reference images and dual pathway processing, but this is implemented as an extension of the existing patterns rather than a deviation from them.

## 7. Key Files to Create/Modify

### 7.1 New Files to Create
- `finetrainers/trainer/e2v_trainer/__init__.py` - Export trainer and configs
- `finetrainers/trainer/e2v_trainer/config.py` - Configuration for E2V training
- `finetrainers/trainer/e2v_trainer/trainer.py` - E2V trainer implementation
- `finetrainers/trainer/e2v_trainer/data.py` - Dataset wrappers for E2V
- `finetrainers/processors/e2v.py` - VAE and CLIP pathway processors

### 7.2 Existing Files to Update
- `finetrainers/config.py` - Add E2V training types to TrainingType enum
- `finetrainers/trainer/__init__.py` - Import and expose E2VTrainer
- `finetrainers/processors/__init__.py` - Import and expose new processors

### 7.3 Tests to Create
- `tests/trainer/test_e2v_trainer.py` - Test E2V trainer functionality
- `tests/processors/test_e2v.py` - Test E2V processors